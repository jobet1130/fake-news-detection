# ğŸ“° Fake News Detection

A full-stack machine learning project that classifies news articles as **real** or **fake** using advanced **Natural Language Processing (NLP)** techniques. This end-to-end pipeline includes classical machine learning, transformer-based deep learning, data preprocessing, model evaluation, and a web-based user interface.

---

## ğŸš€ Project Overview

- **Goal**: Build a robust and scalable pipeline that can accurately detect fake news using both traditional and modern machine learning approaches.
- **Dataset**: [Kaggle â€“ Fake and Real News Dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)
- **Approach**:
  - âœ… Classical ML with TF-IDF + Logistic Regression / Naive Bayes
  - âœ… Ensemble models like XGBoost and Random Forest
  - âœ… Advanced NLP with pretrained transformers (BERT)
  - âœ… Interactive web app using Streamlit for live predictions

---

## ğŸ§± Architecture Overview

```mermaid
graph TD;
    A[Raw Data (Kaggle)] --> B[Preprocessing (Cleaning, Tokenizing)];
    B --> C[Feature Engineering (TF-IDF / Embeddings)];
    C --> D[Model Training (LogReg / XGBoost / BERT)];
    D --> E[Evaluation (Metrics, Reports)];
    D --> F[App Interface (Streamlit)];
```

---

## ğŸ—ƒï¸ Folder Structure

```
fake-news-detection/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/               # Original dataset from Kaggle
â”‚   â”œâ”€â”€ interim/           # Combined and labeled dataset
â”‚   â””â”€â”€ processed/         # Cleaned and transformed data (TF-IDF, etc.)
â”‚
â”œâ”€â”€ notebooks/             # Jupyter notebooks for EDA, modeling, experimentation
â”œâ”€â”€ models/                # Trained models and vectorizers
â”œâ”€â”€ src/                   # Python modules for core logic
â”‚   â”œâ”€â”€ data/              # Data loading and processing scripts
â”‚   â”œâ”€â”€ features/          # Feature extraction methods
â”‚   â”œâ”€â”€ models/            # Training and evaluation logic
â”‚   â””â”€â”€ utils/             # Logging, visualization, and helper functions
â”‚
â”œâ”€â”€ app/                   # Streamlit or Flask-based prediction app
â”œâ”€â”€ scripts/               # CLI scripts for automation
â”œâ”€â”€ tests/                 # Unit and integration tests
â”œâ”€â”€ logs/                  # Training logs
â”œâ”€â”€ reports/               # Evaluation outputs (graphs, reports)
â”œâ”€â”€ configs/               # YAML configuration files
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md              # Project documentation
```

---

## ğŸ“Š Dataset Details

- **Source**: Kaggle ([Fake and Real News Dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset))
- **Files**:
  - `Fake.csv` â€“ Articles labeled as fake
  - `True.csv` â€“ Articles labeled as real
- **Columns**:
  - `title`, `text`, `subject`, `date`

---

## ğŸ§  Techniques & Tools

| Category              | Tools / Techniques                                  |
|-----------------------|------------------------------------------------------|
| Text Preprocessing    | NLTK, regex, stopword removal, lemmatization         |
| Feature Engineering   | TF-IDF, Bag-of-Words, BERT embeddings                |
| Models                | Logistic Regression, Naive Bayes, XGBoost, BERT      |
| Evaluation            | Accuracy, Precision, Recall, F1-score, ROC Curve     |
| Deployment            | Streamlit, Flask, Docker (optional)                  |
| Configuration         | YAML-based modular setup                             |
| Tracking & Logging    | Logging module, output reports, experiment logs      |

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository
```bash
git clone https://github.com/jobet1130/fake-news-detection.git
cd fake-news-detection
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt
```

### 3. Download Dataset
Download the dataset manually from:
> https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset

Place `Fake.csv` and `True.csv` into the `data/raw/` directory.

---

## ğŸ§ª Quick Start Guide

### Preprocess the Data
```bash
python scripts/preprocess_data.py
```

### Train the Model
```bash
python scripts/train.py --config configs/train_config.yaml
```

### Evaluate the Model
```bash
python scripts/evaluate.py
```

### Launch Web Application
```bash
cd app/
streamlit run app.py
```

---

## ğŸ“ˆ Example Output

Outputs are saved in the `reports/` folder:

- ğŸ“Š Confusion Matrix
- ğŸ“ Classification Report
- ğŸ“‰ ROC-AUC Curve
- ğŸ“š Model comparison charts

---

## ğŸ” Configuration Example (`train_config.yaml`)

```yaml
model:
  name: xgboost
  params:
    n_estimators: 200
    max_depth: 4
    learning_rate: 0.1

data:
  max_features: 5000
  test_size: 0.2
  random_state: 42

training:
  epochs: 10
  early_stopping: true
```

---

## ğŸ“Œ Future Enhancements

- [ ] Integration with MLOps (e.g., MLflow or DVC)
- [ ] API deployment with FastAPI
- [ ] Incorporate news publisher metadata
- [ ] Real-time Twitter signal analysis
- [ ] Docker and Kubernetes deployment pipeline

---

## ğŸ‘¨â€ğŸ’» Contributors

- Jobet Casquejo â€“ [`@jobet1130`](https://github.com/jobet1130)

---

## ğŸ“„ License

This project is licensed under the **MIT License**. See the [LICENSE](LICENSE) file for details.
